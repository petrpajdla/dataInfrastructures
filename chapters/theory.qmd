# Theory and method {#sec-theory}

:::{.callout-note}

## @sec-theory introduces: {.unnumbered}

- Definitions of fundamental concepts I am building up on further in the text.
- An overview of theoretical approaches the work is determined and shaped by.
- A discussion of archaeological research as theory- and/or data-driven.
- A commentary on data from the theoretical points presented here.

:::


## Definitions and terminology {#sec-terms}

This section presents a discussion of how I (and others) understand **data** and **data infrastructures**.
Data are a corner stone of this work and looking closely at how scholars, policy makers, and various other stakeholders define data is crucial for further understanding of what is possible and what is not in the process of knowledge building.

### Data {#sec-terms-data}

In the current *information age*, word *data*[^1] is almost omnipresent and it became such a generic term that almost everyone has some notion or idea of what data are.
Most of the formal definitions understand data as building blocks of *information* [e.g. @kitchin2022, 4].
These pieces of information are percieved as pre-factual and pre-analytical.
That is, in contrast to facts, false data are data nonetheless, disproven facts are no longer facts [@rosenberg2013, 17-18].
What more, data need to be analysed and interpreted to make a meaning.
Data are furthermore understood as incontrovertible and non-deconstructible units.
These assumptions migh lead to an impression that data exist in the outside world as entities or phenomena independent of the one observing them.
And this is indeed one way to comprehend data, as *'things given'* that just need to be discoverd [cf. @huggett2020a].
This inductive explanation of data brings in itself a danger that the process of discovering the data is viewed as an atheoretical endeavour.

[^1]: In this text, I adhere to the current scholarly convention as noted by @kitchin2022 [xvii], i.e. using term *data* in the plural form, with a singular form of *datum*.

On the contrary, if data are understood as *'things made'* they are created at the moment when they are captured by observation, measurement, or derived from other data.
This implies theory-laden creative process of data generation, recording, etc.
In this case, it is clear that the one creating or recording the data does that based on the experience, objective and knowledge he/she has.
The whole process of data recording, creation, or capture is thus discursively framed and technically mediated [cf. @kitchin2022, 4-15; @huggett2020a].
As Kitchin notes, what we nowadays label as data are in fact *capta*, i.e. *'things taken'*.

@ruppert2017 voice the idea that the practice of data production does not happen through unstructured social (and/or political) practices, but through structured and structuring processes that add to configurations of power and knowledge.
Hence, I do not understand *data* as ubiquitous phenomena that are waiting out there to be discovered but as *capta* that are actively and creatively made, recorded, generated etc. by a theory- and agenda-laden researchers.
What more, the practice of data production is shaped by and contributing to structure of power and knowledge, because what data are recorded and how they are structured has consequences as to what can be later devised, i.e. what knowlege can be generated.

<!--
[@ruppert2017]:
"(...) the production of data is a social and often political practice that mobilizes agents who are not only objects of data (about whom data is produced) but that they are also subjects of data (those whose engagement drives how data is produced). Our question thus shifts to social practices and agents. Data does not happen through unstructured social practices but through structured and structuring fields in and through which various agents and their interests generate forms of expertise, interpretation, concepts, and methods that collectively function as fields of power and knowledge."

cited in [@kitchin2022]:
"The production of data is a social practice, conducted through structured and structuring fields (e.g. methods, concepts, expertise, institutions) that are shaped by and contribute to configurations of power and knowledge."
-->

If the chosen strategies in data recording and generation influence the knowledge generated further along the way, an idea of good and bad data comes to mind.
<!-- Talking about how chosen strategies in data recording and generation may influence the knowledge generated further along the way an idea of good and bad data might come to mind. -->
Bad data, like false data, do not exist.
Data can be useless, but the value of data is determined by the current goal in mind, data deemed useless by one project can become useful on other occassion, perhaps previously unforeseen and unintended.
Nevertheless, there are some signs of good-quality data as cited[^2] by @kitchin2022 [4]:

- they are discreet and intelligible, i.e. each datum is individual, separate and separable, and clearly defined;
- they are aggregative, that is they can be built into sets;
- they have associated metadata (data about data);
- they can be linked to other datasets to provide insights not available from a single dataset.

[^2]: Author's note: Kitchin cites @rosenberg2013, but I was not able to locate this part in Rosenberg's paper, which is primarily focused on the history of the term *data* in English language, not the quality of data.

<!-- "(...) databases are designed and build to hold certain kinds of data and enable certain kinds of analysis, and how they are structured has profound consequences as to what queries and analysis can be performed." [@ruppert2012] -->

### Infrastructures {#sec-terms-infra}

Although *infrastructures* became quite a buzzword in recent years both among the policy makers and researchers, it is rather difficult to define what an infrastructure actually is.
Many slightly different variations of more-less the same name and concept are circulating in official documents, various reports, research articles etc.
Thus, we encounter terms such as **research infrastructures**, large research infrastructures, open science infrastrucutres, **data infrastructures**, and perhaps more, even though most of their definitions are variations of the very same concept.

@hallonsten2020 maps the field of European (research) infrastructures and identifies the principal problem as the difficulty to come up with a single definition that would fit all of those who are considered to be (or consider themselves to be) an *infrastructure*.
@hallonsten2020 [630] concludes that:

> *(...) "while a politically viable definition seems to be either already in place (...) or unneeded, an analytically workable definition is out of reach unless the scope is limited and the aim of the definition is made more precise."*

In the next paragraphs, we look at several of the *political* definitions of research infrastructures and later we focus on an *analytical* definition of data infrastructures.

#### Political definitions

In the European Union, research infrastructures are currently defined in the Article 2(1) of EU Regulation No 2021/695 establishing Horizon Europe [-@horizon] as facilities providing resources  and services for the research communities in their respective fields. These include:

- human resources, major scientific equipment or sets of instruments;
- collections, archives or scientific data, i.e. knowledge-related facilities;
- computing systems and communication networks;
- any other research and innovation infrastructure of a unique nature open to external users.

The *Regulation* also states that infrastructures may be used beyond research, i.e. for education, public services etc.
This broad definition given by the European Union covers almost any kind of an infrastructure.
In the legal framework of the Czech Republic, given by Act No 130/2002 Coll. on the Support of Research, Experimental Development and Innovation [-@zakon130], Article 2(2), *large research infrastructure* is defined as follow [english translation from Roadmap of Large Research Infrastructures -@roadmap2019]:

> *"(...) a facility necessary for conducting comprehensive research and development with high financial and technology demands, approved by the Government and established to be also used by other research organisations."*

This political definition is rather an opportunistic one in demanding that the infrastructure is approved by the Czech government and has high financial and technology demands.
Lastly, the UNESCO Open Science Recommendation [-@unesco2021] adds to the research infrastructures a strong element of open science, addressing them as *open science infrastructures*.

To conclude, the political definitions of research infrastructures are mostly broad enough to fit any kind of a facility, that is deemed appropriate.
This point is highlighted especially in the Czech definition, where an approval of the Government is required.
In general, there is an emphasis on provision of services (etc.) to various stakeholder communities and cooperation.
In the EU Regulation, a subset of a larger field of infrastructures is labeled *knowledge-related facilities*, it is exactly this part that is discussed here as *data infrastructures*.

#### Data infrastructures

@kitchin2022 [47-48] builds up the definition of data infrastructures by comparing them to *data holdings* and *data archives*.
*Data holdings* are any data stored informally, presumably by an individual (scientist), without long-term preservation or sharing for reuse in mind.
Such data are inevitably lost when the researcher retires, dies (etc.), because proper metadata descriptions are missing and the data, although they might be organised in some way, lack  documentation and it is difficult, if not impossible, to reconstruct the context of the data.
In contrast, *data archives* are formal collections that are structured, curated and documented by appropriate metadata with plans for preservation, access and discoverability.

The role of an interconnected digital world is then highlighted in Kitchin's definition of a data infrastructure [@kitchin2022, 50]:

> *"A data infrastructure is a digital means for storing, sharing, connecting and consuming data holdings and archives across the internet."*

Data infrastructures are thus information systems, repositories, archives, databases etc. shared by multiple shareholder groups that are essential in supporting open science and research.


## Overview of theoretical concepts {#sec-concepts}

This section aims at briefly introducing theoretical approaches this work is shaped by.
At first, digital humanities as an umbrella concept connecting the digital and/or computing world with the humanitites are introduced followed by discussion of digital, quantitative and computational archaeology.
And lastly, spatial archaeology as a main concept framing this study is reviewed.

### Digital humanities

### Digital, computational and quantitative archaeology

### Spatial archaeology


## Archaeology as theory- and data-driven

::: {.callout-note}

## Note

This section is partly based on the *Data-driven Archaeology. Are we there yet?* talk co-authored with Hana Kubelková and Petr Květina presented at the *Central European Theoretical Archaeology Group (CE TAG)* meeting focused on *Theoretical Approaches to Computational Archaeology* I coorganized with Michael Kempf, Jan Kolář and Jiří Macháček in 2021 at the Department of Archaeology and Museology, Faculty of Arts, Masaryk University.

:::

In his vision for the future of archaeology, @kristiansen2014 [14] sees the opposition between theory and data disappear.
Here we examine the dichotomy between data- and theory-driven approaches.
The idea that scientific method and/or theory is dead was addressed by many, but I will start this discussion with a trending comment by @anderson2008.

Anderson uses Boxes famous aphorism *"(...) all models are wrong (...)"* [@box1976, 792], later extended to *"All models are wrong but some [models] are useful."* [@box1979, 202] and extends it to *"All models are wrong, and increasingly you can succeed without them."* [@anderson2008].
By this statement, the author means that science as we know it, i.e. building a hypothesis

<!-- HERE HERE -->

a (research) question can be tackled by accumulating and analysing large quantities of data (or *big data*[^5]) because

[^5]: *Big data* is a phrase often heard in archaeology to address large amounts of data.
In fact, it is a technical term for a data defined by their large volume, velocity, variety, exhaustivity, resolution, etc. [@kitchin2022, 61-74]. For instance some geophysical, 3D, remote sensing, or archaeogenetic and similar data can fit under the definition of big data, but most of archaeological data, like databases of sites and assamblages etc. are not big data by definition, even though they might cover large spatial and/or temporal regions and consist of many records.



## Theorizing archaeological data

Defining archaeological data, micro- to macro-scales;

Earlier in this chapter it was explained that here, data is understood as *capta*, i.e. they are actively and creatively made, recorded, generated etc.



<!-- Here more less starts chapter Method -->

## Assessing data infrastructures {#sec-quality}

In this section I define a framework for an assessment of the quality of data infrastructures.
As there are some signs of good-quality data (as cited from @kitchin2022 [4] in the @sec-terms-data), i.e. they are discreet and intelligible, aggregative, have associated metadata and can be linked to other datasets, by extension, good-quality data infrastructures allow data to be discreet and separable from other data, enable data aggregation, contain metadata and allow linking to other data sources.
This gives us a general idea about requirements for a data infrastrucutre, but is difficult to assess in practice.
These general ideas are concretized and further developed by the FAIR data principles [@wilkinson2016], which are aimed at enhancing the reusability of data holdings with an emphasis on the ability of machines to find and use the data.
The FAIR data principles are measurable what gives us an opportunity to assess how *FAIR* an infrastructure is.

<!-- The CARE principles, on the other hand, are  -->

FAIR data principles, i.e. findability, accessibility, interoperability and reusability, as originally defined by @wilkinson2016 [4], are as follows.
To be *findable*, data and metadata have globally unique and persistent identifiers; are described with rich metadata which include the identifier of the data they describe; and are registered or indexed in a searchable resource.
To be *accessible*, (meta)data are retrievable by the identifier using a standardized (open, free and universally implementable) communications protocol that allows for an authentication and authorization procedure; and metadata are accessible, even when the data are no longer available.
To be *interoperable*, (meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation; contain vocabularies that follow FAIR principles; and include qualified references to other (meta)data.
To be *reusable*, meta(data) are richly described with a plurality of accurate and relevant attributes; are released with a clear and accessible data usage license; are associated with detailed provenance; and meet domain-relevant community standards.

<!-- To be **Findable**:

  - data and metadata have globally unique and persistent identifiers;
  - data are described with rich metadata;
  - metadata include the identifier of the data they describe;
  - (meta)data are registered or indexed in a searchable resource.

To be **Accessible**:

  - (meta)data are retrievable by the identifier using a standardized communications protocol;

    - the protocol is open, free, and universally implementable;
    - the protocol allows for an authentication and authorization procedure;

  - metadata are accessible, even when the data are no longer available.

- To be **Interoperable**:

  - (meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation;
  - (meta)data use vocabularies that follow FAIR principles;
  - (meta)data include qualified references to other (meta)data.

- To be **Reusable**:

  - meta(data) are richly described with a plurality of accurate and relevant attributes;

    - (meta)data are released with a clear and accessible data usage license;
    - (meta)data are associated with detailed provenance;
    - (meta)data meet domain-relevant community standards. -->

With archaeology audiences in mind, the principles are further explained together with tips for implementations etc. by @hollander2019.
Here I build up on @wilkinson2016 and @hollander2019 to come up with a set of formal, i.e. measurable and/or determinable criteria for assessment of data infrastructures.
This framework is then used to evaluate archaeology data infrastructures in the Czech Republic in @sec-data.

### Assessment framework

```{r}
#| label: data-framework

tbl_framework <- read.csv("tbls/framework.csv")

kable_framework <- function(x, data, label) {
  data[grep(x, data$ID), c("ID", "Quality", "Value")] |>
    knitr::kable(col.names = c("ID", label, "Value"), row.names = FALSE)
}
```

Assessment criteria are grouped together according to the FAIR principle they relate to.
@tbl-framework-f lists criteria for findability of resources.
Data should be easy to find by both humans and machines and well documented by metadata in order to be reusable by other researchers.
To be able to use the data object in any way, it must be possible to uniquely identify it, find it, and refer to it (F1).
This implies that an identifier of some sort, preferably persistent, i.e. immutable and long-lasting, is assigned to the resource (data and/or metadata).
Persistent identifiers typically take form of DOIs, Handles, PURLs and URNs to name just a few examples[^3].

Furthermore, it is convenient to be able to locate the resource (preferably on the internet) if the identifier, and possibly prefix of some sort, is known (F2).
Since @marwick2018 explore the lack of data citation and reuse in archaeology and suggest a standard for data citation, one of the criteria is whether the infrastructure makes it easy to cite the resources it publishes (F3).
Another feature that enables findability of data is a rich metadata description (F4--F5).

```{r}
#| label: tbl-framework-f
#| tbl-cap: Framework for quality assessment of data infrastrastructres -- Findability

kable_framework("F", tbl_framework, "Findability")
```

[^3]: Handles and DOIs (Digital Object Identifiers) are composed of a `prefix/suffix` and typically resolved at <https://doi.org/>.
URNs (Uniform Resource Names), in form `urn:namespace:name`, are mostly used in the Semantic Web and are not resolvable, i.e. URNs do not have information about the location of the object.
PURLs (Persistent Uniform Resource Locators) are an extension of URLs and are resolvable.
URNs and (P)URLs are both subsets of URIs (Uniform Resource Identifiers).
See e.g. @ducharme2013 [21-23] for details.

Criteria for accessibility are listed in @tbl-framework-a.
Accessible data are retrievable under well-defined conditions using standardised protocols.
Certification of a data infrastructure guarantees that its repository is trustworthy, the data are stored safely and will be available over a long period of time.
Certifications may include [CoreTrustSeal](https://www.coretrustseal.org/), [nestor seal](https://www.langzeitarchivierung.de/) etc. (A1).
By a standardised exchange protocol (A2.1) a well-documented technology created and maintained by a recognized authority (e.g. World Wide Web Consortium, [W3C](https://www.w3.org/)) is meant.
For example [SPARQL](SPARQL) is a query language for semantic data created by W3C, [OAI-PMH](OAI-PMH), a Protocol for Metadata Harvesting, is created and maintained by the Open Archives Initiative etc.
By a standardised format (A2.2) a machine-readable format is meant, for instance [XML](https://www.w3.org/TR/xml11/), a W3C format for hierarchical data representation, [RDF](https://www.w3.org/TR/rdf-primer/), a W3C standard for semantic data, etc.

For (meta)data to be easily accessible, the policies for the access need to be clearly stated (A3), i.e. definitions of who can access what and when needs to be explicitly communicated, for example existence of differentiated user roles and/or embargo periods.
This gives both the users accessing the data clear instructions on how to access the objects they need, and the users depositing the data sets options to protect sensitive data etc.
Existence of policies how to handle situations if a data object is no longer available (e.g. deleted, superseded etc.) and presence of metadata tombstones is a good practice how to  communicate that a data object existed, but does not anymore (A4).

```{r}
#| label: tbl-framework-a
#| tbl-cap: Framework for quality assessment of data infrastrastructres -- Accessibility

kable_framework("A", tbl_framework, "Accessibility")
```
Interoperability is the ability of the (meta)data to be easily combined with other data sets, @tbl-framework-i lists the interoperability criteria relevant to data infrastructures.
Machine interoperability is closely related to the availability of APIs and their quality and human interoperability derives from the existence and extensiveness of documentation.

To enable interoperability, (meta)data model[^4] needs to be described clearly and accessibly (I1) and employed controlled vocabularies need to be explained and published, preferably following the FAIR principles (I2).
Explanation of the given data model and vocabularies describing exact meanings embedded in the data are a prerequisites for building understanding by other people.
Furthermore, well-documented (meta)data models allow creation of mappings between different metadata schemes and data infrastructures.
Similarly, the existence of machine actionable APIs (application programming interfaces, I4) that allow harvesting of (meta)data through standardised protocols and return responses in standardised formats (cf. A2) ensure machine interoperability.

[^4]: The term *data model* is used here in the sense of how phenomena present, observed and/or measured in the real world are encoded in the data, what ratinale is behind the chosen abstraction process, and what is actually meant by the given wording.

```{r}
#| label: tbl-framework-i
#| tbl-cap: Framework for quality assessment of data infrastrastructres -- Interoperability

kable_framework("I", tbl_framework, "Interoperability")
```

By reusability the process of making data ready for future processing and analysis is meant.
This is crucial for reproducibility of scientific research.
Data, repositories and infrastructures that are systematically documented by manuals, tutorials, guides, codebooks etc. and transparent about what they do and do not contain foster reuse, because researchers reusing the data have clear notion of what to expect from the data source (R1).
Reusability is also enhanced by using widely used and open source file formats (R2).
In the long run, long-term preservation (LTP) is a prerequisite for reusability, because if the file format in which the data is saved gets obsolete, it is often difficult to retrieve the original data, see @brin2013 for recommended file formats, online as *Guides to Good Practice* [-@archaeologydataservice].

Integrity of the (meta)data and existence of multiple versions of the given data objects is also important to consider, because if this information is not properly communicated, different versions of the data objects with identical identifiers might get mixed up (R3).
This closely relates to the provenance of the data, i.e. the documentation of the origin of the data object and record of any changes with a rationale behind these processes.
Knowing why changes in the (meta)data happened, whether it was a correction of a previous mistake or something else, might be useful for data reuse in the future.
Lastly, releasing the (meta)data with proper license information, preferably under a standard data license, for instance a [Creative Commons Licence](https://creativecommons.org/), and any information on a rights holder is neccessary for future reuse because without this information, it is unclear what the terms of (meta)data use are.

```{r}
#| label: tbl-framework-r
#| tbl-cap: Framework for quality assessment of data infrastrastructres -- Reusability

kable_framework("R", tbl_framework, "Reusability")
```
The framework consists predominantly of qualities that are measurable and builds up on the FAIR data principles.
CARE data principles, as defined by @carroll2020, were considered as well, but their goal is to increase the indigenous data sovereignity and self-determination by being people and purpose-oriented, while the FAIR data principles are primarily focused on the characteristics of the data.
CARE data principles are put together to address imbalances of power in the knowledge societies and economies and protect indigenous and human rights.
Hence the extent to which a data infrastructure adheres to CARE data principles is difficult to determine and/or measure.

The framework for assessment of the quality of data infrastructures is used in @sec-data to evaluate the quality of archaeology data infrastructures in the Czech Republic.

## Software

Most of the things included here, if not all of them, were achieved using open-source software.
Large part of this endeavor is also documented in code.
This text was written in plain text with some basic markdown and quarto syntax for formatting, cross references, citations etc.
At some places there are R code blocks.
The text is processed into three outputs, a [website](https://petrpajdla.github.io/dataInfrastructures/) (HTML document), a [PDF](https://petrpajdla.github.io/dataInfrastructures/Archaeology-Data-Infrastructures.pdf) document and a [MS Word](https://petrpajdla.github.io/dataInfrastructures/Archaeology-Data-Infrastructures.docx) document using Quarto.
The plain text version, same as the rendered website, is hosted at [GitHub](https://github.com/petrpajdla/dataInfrastructures).
The text was mostly written in the Visual Code Studio, analysis were mostly performed using Rstudio or terminal.
Library was organized using [Zotero]().

Raster graphics were created and edited using [GIMP](), vector graphics using [Inkscape]().
All the GIS operations that required graphical user interface (GUI), or were more conveniently performed in a GUI, were done in [QGIS]().

Some data were prepared, extracted or processed using basic GNU/Linux shell or SQL commands or scripts.
Data from [Wikidata]() was queried using SPARQL.
Any analysis was mostly done in R, a language for statistical computing and graphics [@rcore].
Various packages were used, the most important packages are listed here, the complete list is in an Appendix


### Reproduciblity


## Chapter summary {.unnumbered}

<!-- One paragraph summarising what is the chapter about. -->
