# Theory and method {#sec-theory}

:::{.callout-note}

## @sec-theory introduces: {.unnumbered}

- Definitions for fundamental concepts I am building upon further in the text.
- An overview of theoretical approaches the work is determined and shaped by.
- A discussion of archaeological research as theory- and/or data-driven.
- A commentary on data from the theoretical points presented earlier.

:::


## Definitions and terminology {#sec-terms}

This section presents a discussion of how I (and others) understand **data** and **data infrastructures**.
Data are a corner stone of this work and looking closely at how scholars, policy makers, and various other stakeholders define them is crucial for further understanding what is possible and what is not in the process of knowledge building.

### Data {#sec-terms-data}

In the current *information age*, word *data*[^1] is almost omnipresent and it became such a generic term that it is somewhat challenging to define.
Most of the definitions understand data as building blocks of *information* [e.g. @kitchin2022, 4].
These pieces of information are percieved as pre-factual and pre-analytical.
That is, in contrast to facts, false data are data nonetheless, disproven facts are no longer facts [@rosenberg2013, 17-18].
Thus data are understood as incontrovertible and non-deconstructibe units.
These assumptions migh lead to an impression that data exist in the outside world as entities or phenomena independent of the one observing them.
And this is indeed one way to comprehend data, as *'things given'* that just need to be discoverd [cf. @huggett2020a].
This inductive explanation of data brings in itself a danger that the process of discovering the data is viewed as an atheoretical endeavour.

[^1]: In this text, I adhere to the current scholarly convention as noted by @kitchin2022 [xvii], i.e. using term *data* in the plural form, with a singular form of *datum*.

On the contrary, data understood as *'things made'* are created at the moment when they are captured by observation, measurement, or derived from other data.
This implies theory-laden creative process of data generation, recording, etc.
In this case, it is clear that the one creating or recording the data does that based on the experience, objective and knowledge he/she has.
The whole process of data recording, creation, or capture is thus discursively framed and technically mediated [cf. @kitchin2022, 4-15; @huggett2020a].
As Kitchin notes, what we label as data are in fact *capta*, i.e. *'things taken'*.

@ruppert2017 voice the idea that the practice of data production does not happen through unstructured social (and/or political) practices, but through structured and structuring fields that add to configurations of power and knowledge.
In this section we went from understanding *data* as ubiquitous phenomena that are waiting out there to be discovered to *capta* that are actively and creatively made, recorded, generated etc. by a theory- and agenda-laden researcher.
What more, the practice of data production is recognized as being shaped by and contributing to structure of power and knowledge.
What data are recorded and how they are structured has consequences as to what can be later devised, i.e. what knowlege can be generated.

<!--
[@ruppert2017]:
"(...) the production of data is a social and often political practice that mobilizes agents who are not only objects of data (about whom data is produced) but that they are also subjects of data (those whose engagement drives how data is produced). Our question thus shifts to social practices and agents. Data does not happen through unstructured social practices but through structured and structuring fields in and through which various agents and their interests generate forms of expertise, interpretation, concepts, and methods that collectively function as fields of power and knowledge."

cited in [@kitchin2022]:
"The production of data is a social practice, conducted through structured and structuring fields (e.g. methods, concepts, expertise, institutions) that are shaped by and contribute to configurations of power and knowledge."
-->

Talking about how chosen strategies in data recording and generation may influence the knowledge generated further along the way an idea of good and bad data might come to mind.
Bad data, like false data, do not exist.
Data can be useless, but their value is probably more determined by the current goal in mind.
Nevertheless, there are some signs of good-quality data as cited[^2] by @kitchin2022 [4]:

- they are discreet and intelligible, i.e. each datum is individual, separate and separable, and clearly defined;
- they are aggregative, that is they can be built into sets;
- they have associated metadata (data about data);
- they can be linked to other datasets to provide insights not available from a single dataset.

[^2]: Author's note: Kitchin cites @rosenberg2013, but I was not able to locate this part in Rosenberg's paper, which is primarily focused on the history of the term *data* in English language, not the quality of data.

<!-- "(...) databases are designed and build to hold certain kinds of data and enable certain kinds of analysis, and how they are structured has profound consequences as to what queries and analysis can be performed." [@ruppert2012] -->

### Infrastructures {#sec-terms-infra}

Although *infrastructures* became quite a buzzword in recent years both among the policy makers and researchers, it is rather difficult to define what an infrastructure actually is.
Many slightly different variations of more-less the same name and concept are circulating in official documents, various reports, research articles etc.
Thus, we encounter terms such as **research infrastructures**, large research infrastructures, open science infrastrucutres, **data infrastructures**, and perhaps more, even though most of their definitions are variations of the very same concept.

@hallonsten2020 maps the field of European (research) infrastructures and identifies the principal problem as the difficulty to come up with a single definition that would fit all of those who are considered to be (or consider themselves to be) an *infrastructure*.
@hallonsten2020 [630] concludes that:

> *(...) "while a politically viable definition seems to be either already in place (...) or unneeded, an analytically workable definition is out of reach unless the scope is limited and the aim of the definition is made more precise."*

In the next paragraphs, we look at several of the *political* definitions of research infrastructures and later we focus on an *analytical* definition of data infrastructures.

#### Political definitions

In the European Union, research infrastructures are currently defined in the Article 2(1) of EU Regulation No 2021/695 establishing Horizon Europe [-@horizon] as facilities providing resources  and services for the research communities in their respective fields. These include:

- human resources, major scientific equipment or sets of instruments;
- collections, archives or scientific data, i.e. knowledge-related facilities;
- computing systems and communication networks;
- any other research and innovation infrastructure of a unique nature open to external users.

The *Regulation* also states that infrastructures may be used beyond research, i.e. for education, public services etc.
This broad definition given by the European Union covers almost any kind of an infrastructure.
In the legal framework of the Czech Republic, given by Act No 130/2002 Coll. on the Support of Research, Experimental Development and Innovation [-@zakon130], Article 2(2), *large research infrastructure* is defined as follow [english translation from Roadmap of Large Research Infrastructures -@roadmap2019]:

> *"(...) a facility necessary for conducting comprehensive research and development with high financial and technology demands, approved by the Government and established to be also used by other research organisations."*

This political definition is rather an opportunistic one in demanding that the infrastructure is approved by the Czech government and has high financial and technology demands.
Lastly, the UNESCO Open Science Recommendation [-@unesco2021] adds to the research infrastructures a strong element of open science, addressing them as *open science infrastructures*.

To conclude, the political definitions of research infrastructures are mostly broad enough to fit any kind of a facility, that is deemed appropriate.
This point is highlighted especially in the Czech definition, where an approval of the Government is required.
In general, there is an emphasis on provision of services (etc.) to various stakeholder communities and cooperation.
In the EU Regulation, a subset of a larger field of infrastructures is labeled *knowledge-related facilities*, it is exactly this part that is discussed here as *data infrastructures*.

#### Data infrastructures

@kitchin2022 [47-48] builds up the definition of data infrastructures by comparing them to *data holdings* and *data archives*.
*Data holdings* are any data stored informally, presumably by an individual (scientist), without long-term preservation or sharing for reuse in mind.
Such data are inevitably lost when the researcher retires, dies (etc.), because proper metadata descriptions are missing and the data, although they might be organised in some way, lack  documentation and it is difficult, if not impossible, to reconstruct the context of the data.
In contrast, *data archives* are formal collections that are structured, curated and documented by appropriate metadata with plans for preservation, access and discoverability.

The role of an interconnected digital world is then highlighted in Kitchin's definition of a data infrastructure [@kitchin2022, 50]:

> *"A data infrastructure is a digital means for storing, sharing, connecting and consuming data holdings and archives across the internet."*

Data infrastructures are thus information systems, repositories, archives, databases etc. shared by multiple shareholder groups that are essential in supporting open science and research.


## Overview of theoretical concepts {#sec-concepts}

This section aims at briefly introducing theoretical approaches this work is shaped by.
At first, digital humanities as an umbrella concept connecting the digital or computing world with the humanitites are introduced followed by discussion of digital, quantitative and computational archaeology.
And lastly, spatial or landscape archaeology as a main concept framing this study is reviewed.

### Digital humanities

### Digital archaeology

### Computational and quantitative archaeology

### Spatial archaeology


## Archaeology as theory- and data-driven

::: {.callout-note}

## Note

This section is partly based on the *Data-driven Archaeology. Are we there yet?* talk co-authored with Hana Kubelková and Petr Květina presented at the *Central European Theoretical Archaeology Group (CE TAG)* meeting focused on *Theoretical Approaches to Computational Archaeology* I coorganized with Michael Kempf, Jan Kolář and Jiří Macháček in 2021 at the Department of Archaeology and Museology, Faculty of Arts, Masaryk University.

:::

In his vision for the future of archaeology, @kristiansen2014 [14] sees the opposition between theory and data disappear.
Here we examine the percieved dichotomy between data- and theory-driven approaches.
The idea that scientific method and/or theory is dead was addressed by many, but I will start this discussion with a trending comment by @anderson2008.

Anderson uses Boxes famous aphorism *"(...) all models are wrong (...)"* [@box1976, 792], later extended to *"All models are wrong but some [models] are useful."* [@box1979, 202] and extends it to *"All models are wrong, and increasingly you can succeed without them."* [@anderson2008].



## Theorizing data

Defining archaeological data, micro- to macro-scales;

Earlier in this chapter it was explained that here, data is understood as *capta*, i.e. *things*, that are actively and creatively made, recorded, generated etc.

  In this section we went from understanding data as ubiquitous phenomena that are waiting out there to be discovered to capta that are actively and creatively made, recorded, generated etc. by a theory- and agenda-laden researcher. What more, the practice of data production is recognized as being shaped by and contributing to structure of power and knowledge. What data are recorded and how they are structured has consequences as to what can be later devised, i.e. what knowlege can be generated.


<!-- Here more less starts chapter Method -->

## Quality of data infrastructures {#sec-quality}

In this section I attempt to come up with a framework for an assessment of the quality of data infrastructures.
As there are some signs of good-quality data (as noted in the @sec-terms-data), i.e. they are discreet and intelligible, aggregative, have associated metadata and can be linked to other datasets, by extension, good-quality data infrastructures allow data to be discreet and separable from other data, enable data aggregation, contain metadata and allow linking to other data sources.
This gives us a general idea about requirements for a data infrastrucutre, but is difficult to assess in practice.
These general ideas are further developed in the FAIR data principles [@wilkinson2016], which are
especially aimed at enhancing the reusability of data holdings with an emphasis on the ability of machines to find and use the data and what more, are measurable.

<!-- The CARE principles, on the other hand, are  -->

<!-- My *framework* for assessing the quality of data infrastructures builds up the concept of FAIR data principles. -->

FAIR data principles, i.e. findability, accessibility, interoperability and reusability, as originally defined by @wilkinson2016 [4], are as follows:

- To be **Findable**:

  - data and metadata have globally unique and persistent identifiers (F1);
  - data are described with rich metadata (F2);
  - metadata include the identifier of the data they describe (F3);
  - (meta)data are registered or indexed in a searchable resource (F4).

- To be **Accessible**:

  - (meta)data are retrievable by the identifier using a standardized communications protocol (A1);

    - the protocol is open, free, and universally implementable (A1.1);
    - the protocol allows for an authentication and authorization procedure (A1.2);

  - metadata are accessible, even when the data are no longer available (A2).

- To be **Interoperable**:

  - (meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation (I1);
  - (meta)data use vocabularies that follow FAIR principles (I2);
  - (meta)data include qualified references to other (meta)data (I3).

- To be **Reusable**:

  - meta(data) are richly described with a plurality of accurate and relevant attributes (R1);

    - (meta)data are released with a clear and accessible data usage license (R1.1);
    - (meta)data are associated with detailed provenance (R1.2);
    - (meta)data meet domain-relevant community standards (R1.3).

With archaeology audiences in mind, the principles are further explained together with tips for implementations etc. by @hollander2019.
Here I build up on @wilkinson2016 and @hollander2019 to come up with a set of formal, i.e. measurable or determinable, criteria for assessment of data infrastructures.
This framework is then used to evaluate archaeology data infrastructures in the Czech Republic in @sec-data.

#### Findability

In order to be able to use the data object in any way, it must be possible to uniquely identify it, find it, and refer to it.
This implies that an identifier of some sort, preferably persistent, i.e. immutable and long-lasting, is assigned to the resource (data and/or metadata).
Another feature that enables findability of data is a rich metadata description.

- Does the data infrastructure have unique identifiers?
- Are the identifiers persistent?
  Are they of any standard form, e.g. DOI, Handle, URN[^3] etc.?
- Is it possible to locate the resource by the identifier?

- Is it possible (and easy) to cite the whole data infrastrucutre, its parts, and individual resources?

[^3]: Persistent identifiers typically take sevral forms.
One example are Handles and DOIs (Digital Object Identifiers) composed of a `prefix/suffix` and typically resolved at <https://doi.org/>.
URNs (Uniform Resource Names), in form `urn:namespace:name`, are mostly used in the Semantic Web and are not resolvable, i.e. URNs do not have information about the location of the object.
PURLs (Persistent Uniform Resource Locators) are an extension of URLs and are resolvable.
URNs and (P)URLs are both subsets of URIs (Uniform Resource Identifiers).
See e.g. @ducharme2013 [21-23] for details on URIs.



## Software

Most of the things included here, if not all of them, were achieved using open-source software.
Large part of this endeavor is also documented in code.
This text was written in plain text with some basic markdown and quarto syntax for formatting, cross references, citations etc.
At some places there are R code blocks.
The text is processed into three outputs, a [website](https://petrpajdla.github.io/dataInfrastructures/) (HTML document), a [PDF](https://petrpajdla.github.io/dataInfrastructures/Archaeology-Data-Infrastructures.pdf) document and a [MS Word](https://petrpajdla.github.io/dataInfrastructures/Archaeology-Data-Infrastructures.docx) document using Quarto.
The plain text version, same as the rendered website, is hosted at [GitHub](https://github.com/petrpajdla/dataInfrastructures).
The text was mostly written in the Visual Code Studio, analysis were mostly performed using Rstudio or terminal.
Library was organized using [Zotero]().

Raster graphics were created and edited using [GIMP](), vector graphics using [Inkscape]().
All the GIS operations that required graphical user interface (GUI), or were more conveniently performed in a GUI, were done in [QGIS]().

Some data were prepared, extracted or processed using basic GNU/Linux shell or SQL commands or scripts.
Data from [Wikidata]() was queried using SPARQL.
Any analysis was mostly done in R, a language for statistical computing and graphics [@rcore].
Various packages were used, the most important packages are listed here, the complete list is in an Appendix


### Reproduciblity


## Chapter summary {.unnumbered}

<!-- One paragraph summarising what is the chapter about. -->
