{
  "hash": "e80cb5393ac146e5b912dbb77a4d2237",
  "result": {
    "engine": "knitr",
    "markdown": "# Theory and method {#sec-theory}\n\n:::{.callout-note}\n\n## @sec-theory introduces: {.unnumbered}\n\n- Definitions of fundamental concepts I am building up on further in the text.\n- An overview of theoretical approaches the work is determined and shaped by.\n- A discussion of archaeological research as theory- and/or data-driven.\n- A commentary on data from the theoretical points presented here.\n\n:::\n\n\n## Definitions and terminology {#sec-terms}\n\nThis section presents a discussion of how I (and others) understand **data** and **data infrastructures**.\nData are a corner stone of this work and looking closely at how scholars, policy makers, and various other stakeholders define data is crucial for further understanding of what is possible and what is not in the process of knowledge building.\n\n### Data {#sec-terms-data}\n\nIn the current *information age*, word *data*[^1] is almost omnipresent and it became such a generic term that almost everyone has some notion or idea of what data are.\nMost of the formal definitions understand data as building blocks of *information* [e.g. @kitchin2022, 4].\nThese pieces of information are percieved as pre-factual and pre-analytical.\nThat is, in contrast to facts, false data are data nonetheless, disproven facts are no longer facts [@rosenberg2013, 17-18].\nWhat more, data need to be analysed and interpreted to make a meaning.\nData are furthermore understood as incontrovertible and non-deconstructible units.\nThese assumptions migh lead to an impression that data exist in the outside world as entities or phenomena independent of the one observing them.\nAnd this is indeed one way to comprehend data, as *'things given'* that just need to be discoverd [cf. @huggett2020a].\nThis inductive explanation of data brings in itself a danger that the process of discovering the data is viewed as an atheoretical endeavour.\n\n[^1]: In this text, I adhere to the current scholarly convention as noted by @kitchin2022 [xvii], i.e. using term *data* in the plural form, with a singular form of *datum*.\n\nOn the contrary, if data are understood as *'things made'* they are created at the moment when they are captured by observation, measurement, or derived from other data.\nThis implies theory-laden creative process of data generation, recording, etc.\nIn this case, it is clear that the one creating or recording the data does that based on the experience, objective and knowledge he/she has.\nThe whole process of data recording, creation, or capture is thus discursively framed and technically mediated [cf. @kitchin2022, 4-15; @huggett2020a].\nAs Kitchin notes, what we nowadays label as data are in fact *capta*, i.e. *'things taken'*.\n\n@ruppert2017 voice the idea that the practice of data production does not happen through unstructured social (and/or political) practices, but through structured and structuring processes that add to configurations of power and knowledge.\nHence, I do not understand *data* as ubiquitous phenomena that are waiting out there to be discovered but as *capta* that are actively and creatively made, recorded, generated etc. by a theory- and agenda-laden researchers.\nWhat more, the practice of data production is shaped by and contributing to structure of power and knowledge, because what data are recorded and how they are structured has consequences as to what can be later devised, i.e. what knowlege can be generated.\n\n<!--\n[@ruppert2017]:\n\"(...) the production of data is a social and often political practice that mobilizes agents who are not only objects of data (about whom data is produced) but that they are also subjects of data (those whose engagement drives how data is produced). Our question thus shifts to social practices and agents. Data does not happen through unstructured social practices but through structured and structuring fields in and through which various agents and their interests generate forms of expertise, interpretation, concepts, and methods that collectively function as fields of power and knowledge.\"\n\ncited in [@kitchin2022]:\n\"The production of data is a social practice, conducted through structured and structuring fields (e.g. methods, concepts, expertise, institutions) that are shaped by and contribute to configurations of power and knowledge.\"\n-->\n\nIf the chosen strategies in data recording and generation influence the knowledge generated further along the way, an idea of good and bad data comes to mind.\n<!-- Talking about how chosen strategies in data recording and generation may influence the knowledge generated further along the way an idea of good and bad data might come to mind. -->\nBad data, like false data, do not exist.\nData can be useless, but the value of data is determined by the current goal in mind, data deemed useless by one project can become useful on other occassion, perhaps previously unforeseen and unintended.\nNevertheless, there are some signs of good-quality data as cited[^2] by @kitchin2022 [4]:\n\n- they are discreet and intelligible, i.e. each datum is individual, separate and separable, and clearly defined;\n- they are aggregative, that is they can be built into sets;\n- they have associated metadata (data about data);\n- they can be linked to other datasets to provide insights not available from a single dataset.\n\n[^2]: Author's note: Kitchin cites @rosenberg2013, but I was not able to locate this part in Rosenberg's paper, which is primarily focused on the history of the term *data* in English language, not the quality of data.\n\n<!-- \"(...) databases are designed and build to hold certain kinds of data and enable certain kinds of analysis, and how they are structured has profound consequences as to what queries and analysis can be performed.\" [@ruppert2012] -->\n\n### Infrastructures {#sec-terms-infra}\n\nAlthough *infrastructures* became quite a buzzword in recent years both among the policy makers and researchers, it is rather difficult to define what an infrastructure actually is.\nMany slightly different variations of more-less the same name and concept are circulating in official documents, various reports, research articles etc.\nThus, we encounter terms such as **research infrastructures**, large research infrastructures, open science infrastrucutres, **data infrastructures**, and perhaps more, even though most of their definitions are variations of the very same concept.\n\n@hallonsten2020 maps the field of European (research) infrastructures and identifies the principal problem as the difficulty to come up with a single definition that would fit all of those who are considered to be (or consider themselves to be) an *infrastructure*.\n@hallonsten2020 [630] concludes that:\n\n> *(...) \"while a politically viable definition seems to be either already in place (...) or unneeded, an analytically workable definition is out of reach unless the scope is limited and the aim of the definition is made more precise.\"*\n\nIn the next paragraphs, we look at several of the *political* definitions of research infrastructures and later we focus on an *analytical* definition of data infrastructures.\n\n**Political definitions**\n\nIn the European Union, research infrastructures are currently defined in the Article 2(1) of EU Regulation No 2021/695 establishing Horizon Europe [-@horizon] as facilities providing resources  and services for the research communities in their respective fields. These include:\n\n- human resources, major scientific equipment or sets of instruments;\n- collections, archives or scientific data, i.e. knowledge-related facilities;\n- computing systems and communication networks;\n- any other research and innovation infrastructure of a unique nature open to external users.\n\nThe *Regulation* also states that infrastructures may be used beyond research, i.e. for education, public services etc.\nThis broad definition given by the European Union covers almost any kind of an infrastructure.\nIn the legal framework of the Czech Republic, given by Act No 130/2002 Coll. on the Support of Research, Experimental Development and Innovation [-@zakon130], Article 2(2), *large research infrastructure* is defined as follow [english translation from Roadmap of Large Research Infrastructures -@roadmap2019]:\n\n> *\"(...) a facility necessary for conducting comprehensive research and development with high financial and technology demands, approved by the Government and established to be also used by other research organisations.\"*\n\nThis political definition is rather an opportunistic one in demanding that the infrastructure is approved by the Czech government and has high financial and technology demands.\nLastly, the UNESCO Open Science Recommendation [-@unesco2021] adds to the research infrastructures a strong element of open science, addressing them as *open science infrastructures*.\n\nTo conclude, the political definitions of research infrastructures are mostly broad enough to fit any kind of a facility, that is deemed appropriate.\nThis point is highlighted especially in the Czech definition, where an approval of the Government is required.\nIn general, there is an emphasis on provision of services (etc.) to various stakeholder communities and cooperation.\nIn the EU Regulation, a subset of a larger field of infrastructures is labeled *knowledge-related facilities*, it is exactly this part that is discussed here as *data infrastructures*.\n\n**Data infrastructures**\n\n@kitchin2022 [47-48] builds up the definition of data infrastructures by comparing them to *data holdings* and *data archives*.\n*Data holdings* are any data stored informally, presumably by an individual (scientist), without long-term preservation or sharing for reuse in mind.\nSuch data are inevitably lost when the researcher retires, dies (etc.), because proper metadata descriptions are missing and the data, although they might be organised in some way, lack  documentation and it is difficult, if not impossible, to reconstruct the context of the data.\nIn contrast, *data archives* are formal collections that are structured, curated and documented by appropriate metadata with plans for preservation, access and discoverability.\n\nThe role of an interconnected digital world is then highlighted in Kitchin's definition of a data infrastructure [@kitchin2022, 50]:\n\n> *\"A data infrastructure is a digital means for storing, sharing, connecting and consuming data holdings and archives across the internet.\"*\n\nData infrastructures are thus information systems, repositories, archives, databases etc. shared by multiple shareholder groups that are essential in supporting open science, research and in our case archaeological heritage management.\n\n\n## Overview of theoretical concepts {#sec-concepts}\n\nThis section aims at briefly introducing theoretical approaches this work is shaped by.\nAt first, digital humanities as an umbrella concept connecting the digital and/or computing world with the humanitites are introduced followed by discussion of digital, quantitative and computational archaeology.\nAnd lastly, spatial archaeology as a main concept framing this study is reviewed.\n\nDigital humanities is now a fully developed interdisciplinary field that utilizes computational methods and digital tools to conduct research, analyze, and interpret humanities data and artefacts.\nDigital humanities does not create qualitatively *new science*, it enhances and extends the ways in which scholars in the humanities approach their research questions with extensive use of computational methods, statistics, geographic information systems, network analyses, text mining etc.\n\nArchaeology, with its strong interdisciplinary focus and early adoption of many innovations, positions itself somewhat aside from the digital humanities.\nThe difference of archaeology might be given by its specific sources, ie. the material culture objects, which is distinct from many of digital humanities disciplines, which are predominantly based on textual sources.\nFieldwork, physical excavations, handling of material culture and focus on physical preservation are processes that lead to archaelogy having apparently closer connections to some of natural sciences, like geology, than to other humanities text-centric disciplines, like history.\nOn the other hand, there are many intersections between digital humanities and archaeology that speak for stronger inclusion of archaeology in this field.\n\n\n<!-- ### Digital, computational and quantitative archaeology\n\n### Spatial archaeology -->\n\n\n## Archaeology as theory- and data-driven\n\n::: {.callout-note}\n\n## Note\n\nThis section is partly based on the *Data-driven Archaeology. Are we there yet?* talk co-authored with Hana Kubelková and Petr Květina presented at the *Central European Theoretical Archaeology Group (CE TAG)* meeting focused on *Theoretical Approaches to Computational Archaeology* I coorganized with Michael Kempf, Jan Kolář and Jiří Macháček in 2021 at the Department of Archaeology and Museology, Faculty of Arts, Masaryk University.\n\n:::\n\nIn his vision for the future of archaeology, @kristiansen2014 [14] sees the opposition between theory and data disappear.\nHere I examine the dichotomy between data- and theory-driven approaches in archaeology, because his work utilises large amounts of data and addressing its data-driven nature is important for methodological transparency and reproducibility, as well as understanding what does it mean in the first place.\nAbundance of data led to an idea, that scientifc method and/or theory is obsolete.\nThis idea was addressed by many, but I will start the discussion with a trending comment by @anderson2008.\n\nAnderson uses Boxes famous aphorism *\"(...) all models are wrong (...)\"* [@box1976, 792], later extended to *\"All models are wrong but some [models] are useful.\"* [@box1979, 202] and extends it to *\"All models are wrong, and increasingly you can succeed without them.\"* [@anderson2008].\nBy this statement, the author means that scientific method as we know it, i.e. positing a research question based on previous knowledge and observation, formulating a testable and falsifiable hypothesis to address this question, and gathering data or conducting experiments in order to test the hypothesis, is superseded by accumulating large quantities of data (or *big data*[^5]) and analyzing it for correlations is good enough, even better, than burdening yourself with building models first.\nAnderson simply proclaims that *\"correlation supersedes causation, and science can advance even without coherent models, unified theories, or really any mechanistic explanation at all\"*.\n\n[^5]: *Big data* is a phrase often heard in archaeology to address large amounts of data.\nIn fact, it is a technical term for a data defined by their large volume, velocity, variety, exhaustivity, resolution, etc. [@kitchin2022, 61-74].\nFor instance some geophysical, 3D, remote sensing, or archaeogenetic and similar data can fit under the definition of big data, but most of archaeological data, like databases of sites and assamblages etc. are not big data by definition, even though they might cover large spatial and/or temporal regions and consist of many records and variables.\n\nBoth theory-driven and data-driven aspects are fundamentally rooted in archaeology.\n\n\n\nArchaeology is often characterized as both theory-driven and data-driven, reflecting the dynamic interplay between conceptual frameworks and empirical evidence in the discipline. Here's an exploration of each aspect:\n\n    Theory-Driven Aspect:\n\n        Conceptual Frameworks: Archaeology is fundamentally rooted in theoretical frameworks that guide research questions, methodologies, and interpretations. Theoretical perspectives, such as cultural ecology, processual archaeology, post-processual archaeology, and more recently, critical theory and postcolonial theory, provide lenses through which archaeologists conceptualize the past and formulate hypotheses.\n\n        Interpretive Models: Theories in archaeology contribute to the development of interpretive models that help make sense of archaeological data. These models can shape how archaeologists understand cultural practices, human behavior, and the socio-economic structures of past societies.\n\n        Research Design: Theoretical considerations influence the design of archaeological research projects, determining the selection of excavation sites, sampling strategies, and the types of data collected. Theories guide decisions on what aspects of the archaeological record are deemed significant and worthy of investigation.\n\n        Hypothesis Testing: Archaeological research often involves the formulation of hypotheses based on theoretical propositions. The data collected during fieldwork and analysis are then used to test and refine these hypotheses, contributing to the iterative nature of archaeological inquiry.\n\n    Data-Driven Aspect:\n\n        Empirical Evidence: Archaeology relies heavily on empirical evidence derived from the material remains of past human activities. This evidence includes artifacts, ecofacts, features, and other archaeological finds. The collection and analysis of these data are central to constructing narratives about past societies.\n\n        Site-Specific Investigations: The data-driven aspect of archaeology emphasizes the importance of site-specific investigations. Fieldwork involves meticulous excavation, documentation, and analysis of artifacts and contexts to generate a detailed understanding of a particular archaeological site.\n\n        Quantitative Methods: While qualitative methods play a significant role, archaeology increasingly incorporates quantitative methods for data analysis. Statistical techniques, GIS (Geographic Information Systems), and other computational tools contribute to the rigorous examination of archaeological data.\n\n        Archaeological Science: The application of scientific techniques, such as radiocarbon dating, archaeobotany, and zooarchaeology, exemplifies the data-driven nature of archaeology. These methods provide precise and objective data, enriching the empirical foundation of archaeological research.\n\nIn summary, the dual nature of archaeology as theory-driven and data-driven underscores the symbiotic relationship between theoretical frameworks and empirical evidence. Theories guide the formulation of research questions and the interpretation of findings, while data, derived from meticulous fieldwork and analysis, continually inform and shape archaeological theories, creating a dynamic and evolving discipline.\n\n\n## Theorizing archaeological data\n\nDefining archaeological data, micro- to macro-scales;\n\nEarlier in this chapter it was explained that here, data is understood as *capta*, i.e. they are actively and creatively made, recorded, generated etc.\n\n\n\n<!-- Here more less starts chapter Method -->\n\n## Assessing data infrastructures {#sec-quality}\n\nIn this section I define a framework for an assessment of the quality of data infrastructures.\nAs there are some signs of good-quality data (as cited from @kitchin2022 [4] in the @sec-terms-data), i.e. they are discreet and intelligible, aggregative, have associated metadata and can be linked to other datasets, by extension, good-quality data infrastructures allow data to be discreet and separable from other data, enable data aggregation, contain metadata and allow linking to other data sources.\nThis gives us a general idea about requirements for a data infrastrucutre, but is difficult to assess in practice.\nThese general ideas are concretized and further developed by the FAIR data principles [@wilkinson2016], which are aimed at enhancing the reusability of data holdings with an emphasis on the ability of machines to find and use the data.\nThe FAIR data principles are measurable what gives us an opportunity to assess how *FAIR* an infrastructure is.\n\n<!-- The CARE principles, on the other hand, are  -->\n\nFAIR data principles, i.e. findability, accessibility, interoperability and reusability, as originally defined by @wilkinson2016 [4], are as follows.\nTo be *findable*, data and metadata have globally unique and persistent identifiers; are described with rich metadata which include the identifier of the data they describe; and are registered or indexed in a searchable resource.\nTo be *accessible*, (meta)data are retrievable by the identifier using a standardized (open, free and universally implementable) communications protocol that allows for an authentication and authorization procedure; and metadata are accessible, even when the data are no longer available.\nTo be *interoperable*, (meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation; contain vocabularies that follow FAIR principles; and include qualified references to other (meta)data.\nTo be *reusable*, meta(data) are richly described with a plurality of accurate and relevant attributes; are released with a clear and accessible data usage license; are associated with detailed provenance; and meet domain-relevant community standards.\n\n<!-- To be **Findable**:\n\n  - data and metadata have globally unique and persistent identifiers;\n  - data are described with rich metadata;\n  - metadata include the identifier of the data they describe;\n  - (meta)data are registered or indexed in a searchable resource.\n\nTo be **Accessible**:\n\n  - (meta)data are retrievable by the identifier using a standardized communications protocol;\n\n    - the protocol is open, free, and universally implementable;\n    - the protocol allows for an authentication and authorization procedure;\n\n  - metadata are accessible, even when the data are no longer available.\n\n- To be **Interoperable**:\n\n  - (meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation;\n  - (meta)data use vocabularies that follow FAIR principles;\n  - (meta)data include qualified references to other (meta)data.\n\n- To be **Reusable**:\n\n  - meta(data) are richly described with a plurality of accurate and relevant attributes;\n\n    - (meta)data are released with a clear and accessible data usage license;\n    - (meta)data are associated with detailed provenance;\n    - (meta)data meet domain-relevant community standards. -->\n\nWith archaeology audiences in mind, the principles are further explained together with tips for implementations etc. by @hollander2019.\nHere I build up on @wilkinson2016 and @hollander2019 to come up with a set of formal, i.e. measurable and/or determinable criteria for assessment of data infrastructures.\nThis framework is then used to evaluate archaeology data infrastructures in the Czech Republic in @sec-data.\n\n### Assessment framework\n\n\n::: {.cell}\n\n:::\n\n\nAssessment criteria are grouped together according to the FAIR principle they relate to.\n@tbl-framework-f lists criteria for findability of resources.\nData should be easy to find by both humans and machines and well documented by metadata in order to be reusable by other researchers.\nTo be able to use the data object in any way, it must be possible to uniquely identify it, find it, and refer to it (F1).\nThis implies that an identifier of some sort, preferably persistent, i.e. immutable and long-lasting, is assigned to the resource (data and/or metadata).\nPersistent identifiers typically take form of DOIs, Handles, PURLs and URNs to name just a few examples[^3].\n\nFurthermore, it is convenient to be able to locate the resource (preferably on the internet) if the identifier, and possibly prefix of some sort, is known (F2).\nSince @marwick2018 explore the lack of data citation and reuse in archaeology and suggest a standard for data citation, one of the criteria is whether the infrastructure makes it easy to cite the resources it publishes (F3).\nAnother feature that enables findability of data is a rich metadata description (F4--F5).\n\n\n::: {#tbl-framework-f .cell tbl-cap='Framework for quality assessment of data infrastrastructres, Findability'}\n::: {.cell-output-display}\n\n\n|ID   |Findability                                              |Value      |\n|:----|:--------------------------------------------------------|:----------|\n|F1.1 |Are there unique identifiers?                            |True/False |\n|F1.2 |Are the identifiers persistent?                          |True/False |\n|F1.3 |Are the identifiers in any standard form?                |True/False |\n|F2   |Is it possible to locate the resource by the identifier? |True/False |\n|F3   |Is it possible (and made easy) to cite:                  |-          |\n|F3.1 |- the data infrastructure,                               |True/False |\n|F3.2 |- its parts and/or                                       |True/False |\n|F3.3 |- individual resources?                                  |True/False |\n|F4.1 |Is the metadata scheme described, i.e. explicit?         |True/False |\n|F4.2 |Does the metadata scheme follow a standard?              |True/False |\n|F5   |Are the metadata searchable?                             |True/False |\n\n\n:::\n:::\n\n\n[^3]: Handles and DOIs (Digital Object Identifiers) are composed of a `prefix/suffix` and typically resolved at <https://doi.org/>.\nURNs (Uniform Resource Names), in form `urn:namespace:name`, are mostly used in the Semantic Web and are not resolvable, i.e. URNs do not have information about the location of the object.\nPURLs (Persistent Uniform Resource Locators) are an extension of URLs and are resolvable.\nURNs and (P)URLs are both subsets of URIs (Uniform Resource Identifiers).\nSee e.g. @ducharme2013 [21-23] for details.\n\nCriteria for accessibility are listed in @tbl-framework-a.\nAccessible data are retrievable under well-defined conditions using standardised protocols.\nCertification of a data infrastructure guarantees that its repository is trustworthy, the data are stored safely and will be available over a long period of time.\nCertifications may include [CoreTrustSeal](https://www.coretrustseal.org/), [nestor seal](https://www.langzeitarchivierung.de/) etc. (A1).\nBy a standardised exchange protocol (A2.1) a well-documented technology created and maintained by a recognized authority (e.g. World Wide Web Consortium, [W3C](https://www.w3.org/)) is meant.\nFor example [SPARQL](SPARQL) is a query language for semantic data created by W3C, [OAI-PMH](OAI-PMH), a Protocol for Metadata Harvesting, is created and maintained by the Open Archives Initiative etc.\nBy a standardised format (A2.2) a machine-readable format is meant, for instance [XML](https://www.w3.org/TR/xml11/), a W3C format for hierarchical data representation, [RDF](https://www.w3.org/TR/rdf-primer/), a W3C standard for semantic data, etc.\n\nFor (meta)data to be easily accessible, the policies for the access need to be clearly stated (A3), i.e. definitions of who can access what and when needs to be explicitly communicated, for example existence of differentiated user roles and/or embargo periods.\nThis gives both the users accessing the data clear instructions on how to access the objects they need, and the users depositing the data sets options to protect sensitive data etc.\nExistence of policies how to handle situations if a data object is no longer available (e.g. deleted, superseded etc.) and presence of metadata tombstones is a good practice how to  communicate that a data object existed, but does not anymore (A4).\n\n\n::: {#tbl-framework-a .cell tbl-cap='Framework for quality assessment of data infrastrastructres, Accessibility'}\n::: {.cell-output-display}\n\n\n|ID   |Accessibility                                                 |Value      |\n|:----|:-------------------------------------------------------------|:----------|\n|A1   |Is the repository trustworthy?                                |True/False |\n|A2.1 |Are the (meta)data retrievable using a standardised protocol? |True/False |\n|A2.2 |Are the metadata in a standardised format?                    |True/False |\n|A3   |Is the access policy clearly stated?                          |True/False |\n|A3.1 |Are there embargo periods?                                    |True/False |\n|A3.2 |Are the access rights differentiated?                         |True/False |\n|A4   |Is the metadata available even after the data is not?         |True/False |\n\n\n:::\n:::\n\nInteroperability is the ability of the (meta)data to be easily combined with other data sets, @tbl-framework-i lists the interoperability criteria relevant to data infrastructures.\nMachine interoperability is closely related to the availability of APIs and their quality and human interoperability derives from the existence and extensiveness of documentation.\n\nTo enable interoperability, (meta)data model[^4] needs to be described clearly and accessibly (I1) and employed controlled vocabularies need to be explained and published, preferably following the FAIR principles (I2).\nExplanation of the given data model and vocabularies describing exact meanings embedded in the data are a prerequisites for building understanding by other people.\nFurthermore, well-documented (meta)data models allow creation of mappings between different metadata schemes and data infrastructures.\nSimilarly, the existence of machine actionable APIs (application programming interfaces, I4) that allow harvesting of (meta)data through standardised protocols and return responses in standardised formats (cf. A2) ensure machine interoperability.\n\n[^4]: The term *data model* is used here in the sense of how phenomena present, observed and/or measured in the real world are encoded in the data, what ratinale is behind the chosen abstraction process, and what is actually meant by the given wording.\n\n\n::: {#tbl-framework-i .cell tbl-cap='Framework for quality assessment of data infrastrastructres, Interoperability'}\n::: {.cell-output-display}\n\n\n|ID   |Interoperability                                  |Value      |\n|:----|:-------------------------------------------------|:----------|\n|I1   |Is the (meta)data model explained and documented? |True/False |\n|I2.1 |Are the vocabularies published and/or well-known? |True/False |\n|I2.2 |Are the vocabularies FAIR?                        |True/False |\n|I3   |Are other metadata referenced properly?           |True/False |\n|I4.1 |Is there a machine-actionable API?                |True/False |\n|I4.2 |Is the API well documented?                       |True/False |\n\n\n:::\n:::\n\n\nBy reusability the process of making data ready for future processing and analysis is meant.\nThis is crucial for reproducibility of scientific research.\nData, repositories and infrastructures that are systematically documented by manuals, tutorials, guides, codebooks etc. and transparent about what they do and do not contain foster reuse, because researchers reusing the data have clear notion of what to expect from the data source (R1).\nReusability is also enhanced by using widely used and open source file formats (R2).\nIn the long run, long-term preservation (LTP) is a prerequisite for reusability, because if the file format in which the data is saved gets obsolete, it is often difficult to retrieve the original data, see @brin2013 for recommended file formats, online as *Guides to Good Practice* [-@archaeologydataservice].\n\nIntegrity of the (meta)data and existence of multiple versions of the given data objects is also important to consider, because if this information is not properly communicated, different versions of the data objects with identical identifiers might get mixed up (R3).\nThis closely relates to the provenance of the data, i.e. the documentation of the origin of the data object and record of any changes with a rationale behind these processes.\nKnowing why changes in the (meta)data happened, whether it was a correction of a previous mistake or something else, might be useful for data reuse in the future.\nLastly, releasing the (meta)data with proper license information, preferably under a standard data license, for instance a [Creative Commons Licence](https://creativecommons.org/), and any information on a rights holder is neccessary for future reuse because without this information, it is unclear what the terms of (meta)data use are.\n\n\n::: {#tbl-framework-r .cell tbl-cap='Framework for quality assessment of data infrastrastructres, Reusability'}\n::: {.cell-output-display}\n\n\n|ID   |Reusability                                           |Value      |\n|:----|:-----------------------------------------------------|:----------|\n|R1   |Are there documentation, manuals, tutorials etc?      |True/False |\n|R2.1 |Are common file formats used?                         |True/False |\n|R2.2 |Are file formats suitable for long-term preservation? |True/False |\n|R3.1 |Is the (meta)data provenance documented?              |True/False |\n|R3.2 |Are there any version control mechanisms in place?    |True/False |\n|R4.1 |Are the rights holders and terms of use clear?        |True/False |\n|R4.2 |Are the resources released under a standard license?  |True/False |\n\n\n:::\n:::\n\nThe framework consists predominantly of qualities that are measurable and builds up on the FAIR data principles.\nCARE data principles, as defined by @carroll2020, were considered as well, but their goal is to increase the indigenous data sovereignity and self-determination by being people and purpose-oriented, while the FAIR data principles are primarily focused on the characteristics of the data.\nCARE data principles are put together to address imbalances of power in the knowledge societies and economies and protect indigenous and human rights.\nHence the extent to which a data infrastructure adheres to CARE data principles is difficult to determine and/or measure.\n\nThe framework for assessment of the quality of data infrastructures is used in @sec-data to evaluate the quality of archaeology data infrastructures in the Czech Republic.\n\n## Software {#sec-software}\n\nMost of the things included here, if not all of them, were achieved using open-source software.\nLarge part of this endeavor is also documented in code.\nThis text was written in plain text with some basic markdown and quarto syntax for formatting, cross references, citations etc.\nAt some places there are R code blocks.\nThe text is processed into three outputs, a [website](https://petrpajdla.github.io/dataInfrastructures/) (HTML document), a [PDF](https://petrpajdla.github.io/dataInfrastructures/Archaeology-Data-Infrastructures.pdf) document and a [MS Word](https://petrpajdla.github.io/dataInfrastructures/Archaeology-Data-Infrastructures.docx) document using Quarto.\nThe plain text version, same as the rendered website, is hosted at [GitHub](https://github.com/petrpajdla/dataInfrastructures).\nThe text was mostly written in the Visual Code Studio, analysis were mostly performed using Rstudio or terminal.\nLibrary was organized using [Zotero]().\n\nRaster graphics were created and edited using [GIMP](), vector graphics using [Inkscape]().\nAll the GIS operations that required graphical user interface (GUI), or were more conveniently performed in a GUI, were done in [QGIS]().\n\nSome data were prepared, extracted or processed using basic GNU/Linux shell or SQL commands or scripts.\nData from [Wikidata]() was queried using SPARQL.\nAny analysis was mostly done in R, a language for statistical computing and graphics [@rcore].\nVarious packages were used, the most important packages are listed here, the complete list is in an Appendix\n\n\n### Reproduciblity\n\n\n## Chapter summary {.unnumbered}\n\n<!-- One paragraph summarising what is the chapter about. -->\n",
    "supporting": [
      "theory_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}